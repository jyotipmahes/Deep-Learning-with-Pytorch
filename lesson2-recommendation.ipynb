{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will write a matrix factorization model in pytorch to solve a recommendation problem. Then we will write a more general neural model for the same problem.\n",
    "\n",
    "Collaborative filtering: systems recommend items based on similarity measures between users and/or items. The items recommended to a user are those preferred by similar users. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MovieLens dataset (ml-latest-small) describes 5-star rating and free-text tagging activity from MovieLens, a movie recommendation service. It contains 100004 ratings and 1296 tag applications across 9125 movies. https://grouplens.org/datasets/movielens/. To get the data:\n",
    "\n",
    "`wget http://files.grouplens.org/datasets/movielens/ml-latest-small.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-10-18 23:15:05--  http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
      "Resolving files.grouplens.org (files.grouplens.org)... 128.101.34.235\n",
      "Connecting to files.grouplens.org (files.grouplens.org)|128.101.34.235|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 978202 (955K) [application/zip]\n",
      "Saving to: ‘ml-latest-small.zip’\n",
      "\n",
      "ml-latest-small.zip 100%[===================>] 955.28K  1.94MB/s    in 0.5s    \n",
      "\n",
      "2018-10-18 23:15:06 (1.94 MB/s) - ‘ml-latest-small.zip’ saved [978202/978202]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://files.grouplens.org/datasets/movielens/ml-latest-small.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lesson1-intro.ipynb\t      ml-latest-small.zip\r\n",
      "lesson2-recommendation.ipynb  README.md\r\n"
     ]
    }
   ],
   "source": [
    "!ls\n",
    "import zipfile as zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/home/jyoti/Deep-Learning-with-Pytorch/\"\n",
    "zip_ref = zip.ZipFile(path+\"ml-latest-small.zip\", 'r')\n",
    "zip_ref.extractall(path)\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lesson1-intro.ipynb\t      ml-latest-small\t   README.md\r\n",
      "lesson2-recommendation.ipynb  ml-latest-small.zip\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the zip file has been extracted correctly in the same folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MovieLens dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/jyoti/Deep-Learning-with-Pytorch/ml-latest-small/movies.csv'),\n",
       " PosixPath('/home/jyoti/Deep-Learning-with-Pytorch/ml-latest-small/links.csv'),\n",
       " PosixPath('/home/jyoti/Deep-Learning-with-Pytorch/ml-latest-small/ratings.csv'),\n",
       " PosixPath('/home/jyoti/Deep-Learning-with-Pytorch/ml-latest-small/tags.csv'),\n",
       " PosixPath('/home/jyoti/Deep-Learning-with-Pytorch/ml-latest-small/README.txt')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = Path(\"/home/jyoti/Deep-Learning-with-Pytorch/ml-latest-small/\")\n",
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId,movieId,rating,timestamp\r",
      "\r\n",
      "1,1,4.0,964982703\r",
      "\r\n",
      "1,3,4.0,964981247\r",
      "\r\n",
      "1,6,4.0,964982224\r",
      "\r\n",
      "1,47,5.0,964983815\r",
      "\r\n",
      "1,50,5.0,964982931\r",
      "\r\n",
      "1,70,3.0,964982400\r",
      "\r\n",
      "1,101,5.0,964980868\r",
      "\r\n",
      "1,110,4.0,964982176\r",
      "\r\n",
      "1,151,5.0,964984041\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! head /home/jyoti/Deep-Learning-with-Pytorch/ml-latest-small/ratings.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading a csv into pandas\n",
    "data = pd.read_csv(PATH/\"ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding data\n",
    "We enconde the data to have contiguous ids for users and movies. You can think about this as a categorical encoding of our two categorical variables userId and movieId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and validation before encoding\n",
    "np.random.seed(3)\n",
    "msk = np.random.rand(len(data)) < 0.8\n",
    "train = data[msk].copy()\n",
    "val = data[~msk].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is a handy function modified from fast.ai\n",
    "def proc_col(col, train_col=None):\n",
    "    \"\"\"Encodes a pandas column with continous ids. \n",
    "    \"\"\"\n",
    "    if train_col is not None:\n",
    "        uniq = train_col.unique()\n",
    "    else:\n",
    "        uniq = col.unique()\n",
    "    name2idx = {o:i for i,o in enumerate(uniq)}\n",
    "    return name2idx, np.array([name2idx.get(x, -1) for x in col]), len(uniq) #-1 if name is not present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(df, train=None):\n",
    "    \"\"\" Encodes rating data with continous user and movie ids. \n",
    "    If train is provided, encodes df with the same encoding as train.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    for col_name in [\"userId\", \"movieId\"]:\n",
    "        train_col = None\n",
    "        if train is not None:\n",
    "            train_col = train[col_name]\n",
    "        _,col,_ = proc_col(df[col_name], train_col)\n",
    "        df[col_name] = col\n",
    "        print(\"Not found\",len(df[df[col_name] <= 0]))\n",
    "        df = df[df[col_name] >= 0] #Keep only ids which have name2idx\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found 193\n",
      "Not found 178\n",
      "Not found 39\n",
      "Not found 832\n",
      "   userId  movieId  rating  timestamp\n",
      "0       0        0     4.0  964982703\n",
      "1       0        1     4.0  964981247\n",
      "2       0        2     4.0  964982224\n",
      "3       0        3     5.0  964983815\n",
      "6       0        4     5.0  964980868\n",
      "    userId  movieId  rating  timestamp\n",
      "4        0      388     5.0  964982931\n",
      "5        0      995     3.0  964982400\n",
      "29       0      841     4.0  964981179\n",
      "30       0      567     4.0  964982653\n",
      "32       0      402     4.0  964982546\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "80450"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = encode_data(train)\n",
    "df_val = encode_data(val, train)\n",
    "print(df_train.head())\n",
    "print(df_val.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.5182, -0.2305,  1.1904],\n",
       "        [ 2.0218,  0.0332,  0.3650],\n",
       "        [ 0.1089,  1.0361, -0.3134],\n",
       "        [-1.5838,  0.5323, -0.7510],\n",
       "        [-0.5565,  0.2681, -0.5809],\n",
       "        [-0.3779,  1.5249, -0.3305],\n",
       "        [ 1.5884, -0.3437,  0.9804],\n",
       "        [ 0.0469,  0.5491,  0.5290],\n",
       "        [ 0.3627, -0.6631, -1.3172],\n",
       "        [-0.2401, -1.0836,  0.7456]], requires_grad=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an Embedding module containing 10 users or items embedding size 3\n",
    "# embedding will be initialized at random\n",
    "embed = nn.Embedding(10, 3)\n",
    "embed.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.0218,  0.0332,  0.3650],\n",
       "         [-0.5182, -0.2305,  1.1904],\n",
       "         [ 2.0218,  0.0332,  0.3650],\n",
       "         [-0.5565,  0.2681, -0.5809],\n",
       "         [-0.3779,  1.5249, -0.3305],\n",
       "         [ 2.0218,  0.0332,  0.3650]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# given a list of ids we can \"look up\" the embedding corresponing to each id\n",
    "# can you see that some vectors are the same?\n",
    "a = torch.LongTensor([[1,0,1,4,5,1]])\n",
    "embed(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix factorization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, emb_size=100):\n",
    "        super(MF, self).__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, emb_size)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_size)\n",
    "        # initlializing weights\n",
    "        self.user_emb.weight.data.uniform_(0,0.05) #Why we need initialization\n",
    "        self.item_emb.weight.data.uniform_(0,0.05)\n",
    "        \n",
    "    def forward(self, u, v):\n",
    "        u = self.user_emb(u)\n",
    "        v = self.item_emb(v)\n",
    "        return (u*v).sum(1)   #Element wise multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging MF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = 7\n",
    "num_items = 4\n",
    "emb_size = 3\n",
    "\n",
    "user_emb = nn.Embedding(num_users, emb_size)\n",
    "item_emb = nn.Embedding(num_items, emb_size)\n",
    "users = torch.LongTensor(df_t_e.userId.values)\n",
    "items = torch.LongTensor(df_t_e.movieId.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'users' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-2ef8d70d8c36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'users' is not defined"
     ]
    }
   ],
   "source": [
    "U = user_emb(users)\n",
    "V = item_emb(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2887, -0.1039, -0.6517],\n",
       "        [ 0.2887, -0.1039, -0.6517],\n",
       "        [-0.7562,  0.7185, -2.2700],\n",
       "        [-0.7562,  0.7185, -2.2700],\n",
       "        [ 1.6527, -0.2885,  0.0281],\n",
       "        [ 1.6527, -0.2885,  0.0281],\n",
       "        [-1.0987, -1.5382,  0.3912],\n",
       "        [-1.0987, -1.5382,  0.3912],\n",
       "        [-2.2866, -0.6564, -0.5094],\n",
       "        [-2.2866, -0.6564, -0.5094],\n",
       "        [ 0.1742, -1.2741,  0.6683],\n",
       "        [-0.1845, -1.2902, -0.1542],\n",
       "        [-0.1845, -1.2902, -0.1542]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4151,  0.0906,  0.1340],\n",
       "        [-0.2531,  0.0403, -0.3552],\n",
       "        [ 0.6629, -0.2785, -1.2372],\n",
       "        [ 0.1777, -0.8371, -1.0000],\n",
       "        [-2.3761,  0.2516, -0.0058],\n",
       "        [-1.4488,  0.1118,  0.0153],\n",
       "        [ 1.5797,  1.3414, -0.0805],\n",
       "        [ 0.7645, -0.0590, -0.1178],\n",
       "        [ 3.2875,  0.5724,  0.1048],\n",
       "        [ 1.5910, -0.0252,  0.1533],\n",
       "        [-0.1212, -0.0489, -0.2012],\n",
       "        [ 0.1617,  0.5000, -0.0840],\n",
       "        [ 0.1283, -0.0495,  0.0464]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# element wise multiplication\n",
    "U*V "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1905, -0.5680, -0.8528, -1.6593, -2.1303, -1.3217,  2.8406,\n",
       "         0.5877,  3.9646,  1.7191, -0.3713,  0.5777,  0.1252])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what we want is a dot product per row\n",
    "(U*V).sum(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training MF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610 8998\n"
     ]
    }
   ],
   "source": [
    "num_users = len(df_train.userId.unique())\n",
    "num_items = len(df_train.movieId.unique())\n",
    "print(num_users, num_items) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MF(num_users, num_items, emb_size=100)  # if you have a GPU .cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are not using data loaders because our data fits well in memory\n",
    "def train_epocs(model, epochs=10, lr=0.01, wd=0.0, unsqueeze=False):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    model.train()\n",
    "    for i in range(epochs):\n",
    "        users = torch.LongTensor(df_train.userId.values)  #.cuda()\n",
    "        items = torch.LongTensor(df_train.movieId.values) #.cuda()\n",
    "        ratings = torch.FloatTensor(df_train.rating.values)  #.cuda()\n",
    "        if unsqueeze:\n",
    "            ratings = ratings.unsqueeze(1)\n",
    "        y_hat = model(users, items)\n",
    "        loss = F.mse_loss(y_hat, ratings)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(loss.item()) # used to be loss.data[0]\n",
    "    test_loss(model, unsqueeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80450])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([80450, 1])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is what unsqueeze does\n",
    "ratings = torch.FloatTensor(df_train.rating.values)\n",
    "print(ratings.shape)\n",
    "ratings = ratings.unsqueeze(1) #.cuda()\n",
    "ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loss(model, unsqueeze=False):\n",
    "    model.eval() #Explicitly letting the model know that it is a evaluation and not training\n",
    "    users = torch.LongTensor(df_val.userId.values) # .cuda()\n",
    "    items = torch.LongTensor(df_val.movieId.values) #.cuda()\n",
    "    ratings = torch.FloatTensor(df_val.rating.values) #.cuda()\n",
    "    if unsqueeze:\n",
    "        ratings = ratings.unsqueeze(1)\n",
    "    y_hat = model(users, items)\n",
    "    loss = F.mse_loss(y_hat, ratings)\n",
    "    print(\"test loss %.3f \" % loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6454849243164062\n",
      "5.6897969245910645\n",
      "4.107996463775635\n",
      "1.0472257137298584\n",
      "2.813699960708618\n",
      "2.506274700164795\n",
      "0.7590300440788269\n",
      "1.2165346145629883\n",
      "2.0785794258117676\n",
      "1.9998751878738403\n",
      "test loss 1.438 \n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2020612955093384\n",
      "0.8295372724533081\n",
      "0.6758217215538025\n",
      "0.677545964717865\n",
      "0.7039048075675964\n",
      "0.6925514340400696\n",
      "0.6531720161437988\n",
      "0.6152136325836182\n",
      "0.5976650714874268\n",
      "0.6002582907676697\n",
      "0.6099865436553955\n",
      "0.613412618637085\n",
      "0.6045016050338745\n",
      "0.5854303240776062\n",
      "0.5633542537689209\n",
      "test loss 0.787 \n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=15, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5459926128387451\n",
      "0.5513851046562195\n",
      "0.5175593495368958\n",
      "0.5068127512931824\n",
      "0.5004147887229919\n",
      "0.4849693179130554\n",
      "0.4688773453235626\n",
      "0.455656498670578\n",
      "0.44224584102630615\n",
      "0.4272359609603882\n",
      "0.4124053120613098\n",
      "0.3983566462993622\n",
      "0.3839205205440521\n",
      "0.36875271797180176\n",
      "0.353739857673645\n",
      "test loss 0.790 \n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=15, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MF with bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF_bias(nn.Module):\n",
    "    def __init__(self, num_users, num_items, emb_size=100):\n",
    "        super(MF_bias, self).__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, emb_size)\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_size)\n",
    "        self.item_bias = nn.Embedding(num_items, 1)\n",
    "        # init \n",
    "        self.user_emb.weight.data.uniform_(0,0.05)\n",
    "        self.item_emb.weight.data.uniform_(0,0.05)\n",
    "        self.user_bias.weight.data.uniform_(-0.01,0.01)\n",
    "        self.item_bias.weight.data.uniform_(-0.01,0.01)\n",
    "        \n",
    "    def forward(self, u, v):\n",
    "        U = self.user_emb(u)\n",
    "        V = self.item_emb(v)\n",
    "        b_u = self.user_bias(u).squeeze()\n",
    "        b_v = self.item_bias(v).squeeze()\n",
    "        return (U*V).sum(1) +  b_u  + b_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MF_bias(num_users, num_items, emb_size=100) #.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.909826278686523\n",
      "4.196595668792725\n",
      "3.5819344520568848\n",
      "2.319197654724121\n",
      "0.7545202970504761\n",
      "1.8002912998199463\n",
      "2.476569890975952\n",
      "2.0964980125427246\n",
      "1.2626861333847046\n",
      "0.9100744128227234\n",
      "test loss 1.468 \n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.1, wd=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2451554536819458\n",
      "0.8298717737197876\n",
      "0.6669487953186035\n",
      "0.6649450659751892\n",
      "0.7212553024291992\n",
      "0.7655099630355835\n",
      "0.7730416059494019\n",
      "0.7484903335571289\n",
      "0.707660436630249\n",
      "0.6671319007873535\n",
      "test loss 0.778 \n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.01, wd=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6388686299324036\n",
      "0.6308144330978394\n",
      "0.6240432858467102\n",
      "0.6184990406036377\n",
      "0.6140540242195129\n",
      "0.6105394959449768\n",
      "0.6077687740325928\n",
      "0.605559766292572\n",
      "0.6037524342536926\n",
      "0.6022184491157532\n",
      "test loss 0.758 \n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.001, wd=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6008635759353638\n",
      "0.5988619923591614\n",
      "0.5973002314567566\n",
      "0.5958811044692993\n",
      "0.5944750308990479\n",
      "0.5930507779121399\n",
      "0.5916163325309753\n",
      "0.5901870727539062\n",
      "0.5887723565101624\n",
      "0.5873723030090332\n",
      "test loss 0.757 \n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.001, wd=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these models are susceptible to weight initialization, optimization algorithm and regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note here there is no matrix multiplication, we could potentially make the embeddings \n",
    "# for users and items of different sizes.\n",
    "# Here we could get better results by keep playing with regularization.\n",
    "    \n",
    "class CollabFNet(nn.Module):\n",
    "    def __init__(self, num_users, num_items, emb_size=100, n_hidden=10):\n",
    "        super(CollabFNet, self).__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, emb_size)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_size)\n",
    "        self.lin1 = nn.Linear(emb_size*2, n_hidden) # we take 2 because later we are going to concat user and item and feed it as input\n",
    "        self.lin2 = nn.Linear(n_hidden, 1)\n",
    "        self.drop1 = nn.Dropout(0.1)\n",
    "        self.drop2 = nn.Dropout(0.0)\n",
    "        \n",
    "    def forward(self, u, v):\n",
    "        U = self.user_emb(u)\n",
    "        V = self.item_emb(v)\n",
    "        x = torch.cat([U, V], dim=1) #Concating the features and feeding to network\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.drop2(x)\n",
    "        x = self.lin2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CollabFNet(num_users, num_items, emb_size=100) #.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.771885871887207\n",
      "5.834911823272705\n",
      "10.26247501373291\n",
      "1.8494447469711304\n",
      "2.24432373046875\n",
      "2.5424840450286865\n",
      "2.2457563877105713\n",
      "2.140498161315918\n",
      "1.7329623699188232\n",
      "1.2860621213912964\n",
      "1.1354800462722778\n",
      "1.0819933414459229\n",
      "1.0054795742034912\n",
      "0.9785750508308411\n",
      "0.9333520531654358\n",
      "0.8680918216705322\n",
      "0.8539161682128906\n",
      "0.7914043664932251\n",
      "0.785370945930481\n",
      "0.7429141998291016\n",
      "test loss 0.959 \n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=20, lr=0.1, wd=1e-6, unsqueeze=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7443450093269348\n",
      "0.9130212068557739\n",
      "0.7124481201171875\n",
      "0.7190241813659668\n",
      "0.7762979865074158\n",
      "0.7371277809143066\n",
      "0.6773689985275269\n",
      "0.6637667417526245\n",
      "0.692821741104126\n",
      "0.6934863328933716\n",
      "0.6671110391616821\n",
      "0.6384292244911194\n",
      "0.6416347026824951\n",
      "0.6559163928031921\n",
      "0.6541212797164917\n",
      "0.6409287452697754\n",
      "0.6235097646713257\n",
      "0.62278813123703\n",
      "0.6308428049087524\n",
      "0.6326353549957275\n",
      "test loss 0.873 \n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=20, lr=0.01, wd=1e-6, unsqueeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6213779449462891\n",
      "0.6156904697418213\n",
      "0.6094982624053955\n",
      "0.6145181655883789\n",
      "0.6146385669708252\n",
      "0.6145569086074829\n",
      "0.6110187768936157\n",
      "0.612385094165802\n",
      "0.6096471548080444\n",
      "0.6068136692047119\n",
      "test loss 0.854 \n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.001, wd=1e-6, unsqueeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6100354790687561\n",
      "0.6111634373664856\n",
      "0.6062862277030945\n",
      "0.606310248374939\n",
      "0.606798529624939\n",
      "0.6073122024536133\n",
      "0.6038792133331299\n",
      "0.6040692925453186\n",
      "0.603810727596283\n",
      "0.6053792238235474\n",
      "0.6031851172447205\n",
      "0.6048499345779419\n",
      "0.6034085154533386\n",
      "0.601294219493866\n",
      "0.6003644466400146\n",
      "0.6020768880844116\n",
      "0.6012876033782959\n",
      "0.5980045795440674\n",
      "0.5987824201583862\n",
      "0.5970166325569153\n",
      "test loss 0.858 \n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=20, lr=0.001, wd=1e-6, unsqueeze=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "* use t-sne to visualize embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab\n",
    "* Can you use `tags.csv` and `timestamp` to improve your predictions?\n",
    "* Play with the hyperparameters\n",
    "* Look at fastai version of this network and try his transformation https://github.com/fastai/fastai/blob/master/courses/dl1/lesson5-movielens.ipynb\n",
    "* You may need a dataloader if you data is larger. Can you construct a dataset? Here is an example:\n",
    "https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel.html\n",
    "* Work with the largest dataset http://files.grouplens.org/datasets/movielens/ml-latest.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "* This notebook is based on [lesson 5 of Jeremy Howard's Deep Learning Course](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson5-movielens.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
